---
title: "Slides"
number-sections: true
---

*The following tutorials are mainly based on the excellent course ["Building Systems with the ChatGPT API"](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/) provided by Isa Fulford from OpenAI and Andrew Ng from DeepLearning.AI.*


::: {.callout-note appearance="simple"}
Take a look at the [slides tutorial](https://kirenz.github.io/lab-toolkit/slides/slides.html#/title-slide) to learn how to use all slide options. 
:::

You have several options to start code development:

1. **Colab**: Click on one of the links "üíª Jupyter Notebook" to start a Colab session. 

2. **Local**: Click on one of the links "üíª Jupyter Notebook" below, go to the Colab menu and choose "File" > "Download" > "Download .ipynb"

3. **Cloud Codespace**: Work in a fully configured dev environment in the cloud with a [GitHub Codespace VS Code Browser](https://github.com/kirenz/lab-chat-system/blob/main/README.md) environment.

4. **Local VS Code with Codespace**: Use [GitHub Codespaces in your local Visual Studio Code environment](https://docs.github.com/en/codespaces/developing-in-codespaces/using-github-codespaces-in-visual-studio-code).


For cost reasons we mainly use OpenAI's `gpt-3.5-turbo` model in our tutorials. However, you can simply replace `model="gpt-3.5-turbo"` with `model="gpt-4"` in the helper function. Note the [price difference](https://openai.com/pricing) between the two models.


## Basics: Tokens and chat format

In this tutorial, you'll learn some basic properties of Large Language Models (tokens and the chat format):

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/basics.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/basics.ipynb)
:::


## Classification 

Learn how to classify different customer queries:

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/classification.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/classification.ipynb)
:::

## Moderation

The moderations endpoint is a tool you can use to check whether content complies with our usage policies:


::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/moderation.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/moderation.ipynb)
:::

## Chain of Thought Reasoning

Chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding:

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/reasoning.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/reasoning.ipynb)
:::

## Prompt Chaining

Prompt Chaining is a technique of connecting multiple tasks to generate complex content by breaking down a large generative task into smaller, more manageable pieces. Examples of prompt chaining are generating product descriptions, creating personalized email campaigns, and generating complex chatbot responses.


::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/chaining.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/chaining.ipynb)
:::

## Check Model Output 

Ask the model to verify its own outputs:

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/outputcheck.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/outputcheck.ipynb)
:::

## Build an End-to-End System

This puts together the chain of prompts that you saw throughout the tutorials:


::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/system.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/system.ipynb)
- [products.json](/slides/products.json)
- [utils.py](/slides/utils.py)
:::


## Evaluation

Evaluate LLM responses when there is a single "right answer".

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/evaluation.qmd)
- [üíª evaluation.ipynb](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/evaluation.ipynb)
- [products.json](/slides/products.json)
- [utils.py](/slides/utils_2.py)
:::

## Evaluation Part 2

Evaluate LLM responses where there isn't a single "right answer."

::: {.callout-tip appearance="simple" icon=false}
- [üñ•Ô∏è Presentation](/slides/evaluation_2.qmd)
- [üíª Jupyter Notebook](https://colab.research.google.com/github/kirenz/lab-chat-system/blob/main/code/evaluation_2.ipynb)
:::

Resources: 

- OpenAI's [Evals](https://github.com/openai/evals/tree/main) is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks (An "eval" is a task used to evaluate the quality of a system's behavior).